{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_TRACING_V2=True\n",
    "langchain_project  = os.getenv('LANGCHAIN_PROJECT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data using url\n",
    "\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url = url, extractor= lambda x: Soup(x,'html.parser').text, max_depth=20\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'Conceptual guide | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConceptual guide | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual GuideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (rag)RetrievalRetrieversRunnable interfaceStreamingStructured outputsString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy langchain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityConceptual GuideOn this pageConceptual guide\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\nWe recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples ‚Äî those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference.\\nHigh level‚Äã\\n\\nWhy LangChain?: Overview of the value that LangChain provides.\\nArchitecture: How packages are organized in the LangChain ecosystem.\\n\\nConcepts‚Äã\\n\\nChat models: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\nMessages: The unit of communication in chat models, used to represent model input and output.\\nChat history: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\nTools: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\nTool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\nStructured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\nMemory: Information about a conversation that is persisted so that it can be used in future conversations.\\nMultimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\\nDocument loaders: Load a source as a list of documents.\\nRetrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\nText splitters: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\nEmbedding models: Models that represent data such as text or images in a vector space.\\nVector stores: Storage of and efficient search over vectors and associated metadata.\\nRetriever: A component that returns relevant documents from a knowledge base in response to a query.\\nRetrieval Augmented Generation (RAG): A technique that enhances language models by combining them with external knowledge bases.\\nAgents: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool.\\nPrompt templates: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\nOutput parsers: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs.\\nFew-shot prompting: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\nExample selectors: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\nAsync programming: The basics that one should know to use LangChain in an asynchronous context.\\nCallbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\nTracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\nEvaluation: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n\\nGlossary‚Äã\\n\\nAIMessageChunk: A partial response from an AI message. Used when streaming responses from a chat model.\\nAIMessage: Represents a complete response from an AI model.\\nastream_events: Stream granular information from LCEL chains.\\nBaseTool: The base class for all tools in LangChain.\\nbatch: Use to execute a runnable with batch inputs a Runnable.\\nbind_tools: Allows models to interact with tools.\\nCaching: Storing results to avoid redundant calls to a chat model.\\nChat models: Chat models that handle multiple data modalities.\\nConfigurable runnables: Creating configurable Runnables.\\nContext window: The maximum size of input a chat model can process.\\nConversation patterns: Common patterns in chat interactions.\\nDocument: LangChain\\'s representation of a document.\\nEmbedding models: Models that generate vector embeddings for various data types.\\nHumanMessage: Represents a message from a human user.\\nInjectedState: A state injected into a tool function.\\nInjectedStore: A store that can be injected into a tool for data persistence.\\nInjectedToolArg: Mechanism to inject arguments into tool functions.\\ninput and output types: Types used for input and output in Runnables.\\nIntegration packages: Third-party packages that integrate with LangChain.\\ninvoke: A standard method to invoke a Runnable.\\nJSON mode: Returning responses in JSON format.\\nlangchain-community: Community-driven components for LangChain.\\nlangchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\\nlangchain: A package for higher level components (e.g., some pre-built chains).\\nlanggraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\nlangserve: Use to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\nManaging chat history: Techniques to maintain and manage the chat history.\\nOpenAI format: OpenAI\\'s message format for chat models.\\nPropagation of RunnableConfig: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\nrate-limiting: Client side rate limiting for chat models.\\nRemoveMessage: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\nrole: Represents the role (e.g., user, assistant) of a chat message.\\nRunnableConfig: Use to pass run time information to Runnables (e.g., run_name, run_id, tags, metadata, max_concurrency, recursion_limit, configurable).\\nStandard parameters for chat models: Parameters such as API key, temperature, and max_tokens,\\nstream: Use to stream output from a Runnable or a graph.\\nTokenization: The process of converting data into tokens and vice versa.\\nTokens: The basic unit that a language model reads, processes, and generates under the hood.\\nTool artifacts: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\nTool binding: Binding tools to models.\\n@tool: Decorator for creating tools in LangChain.\\nToolkits: A collection of tools that can be used together.\\nToolMessage: Represents a message that contains the results of a tool execution.\\nVector stores: Datastores specialized for storing and efficiently searching vector embeddings.\\nwith_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\nwith_types: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\nEdit this pageWas this page helpful?PreviousHow to create and query vector storesNextAgentsHigh levelConceptsGlossaryCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the docs into chunks\n",
    "text_splitter =  RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstoe aand converting chunks of data into emebeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding= OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing the vectorstore.\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building RAG chain\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "class RagBot:\n",
    "    def __init__(self, retriever, model:str = 'gpt-4-0125-preview'):\n",
    "        self.retriever =  retriever\n",
    "        self.client = wrap_openai(openai.Client())\n",
    "        self.model = model\n",
    "\n",
    "    @traceable\n",
    "    def retrieve_docs(self, question):\n",
    "        return self.retriever.invoke(question)\n",
    "    \n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        similar = self.retrieve_docs(question)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI code assistant with expertise in LCEL.\"\n",
    "                    \" Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{similar}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "    \n",
    "rag_bot = RagBot(retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  rag_bot.get_answer(\"What is LCEL?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The LangChain Expression Language (LCEL) is a syntax designed for orchestrating LangChain components. It is particularly useful for creating simpler applications within the LangChain framework. LCEL enables developers to define and execute complex operations involving various LangChain components, such as document loaders, retrieval systems, chat models, and tools, in a more streamlined and efficient manner. This syntax facilitates the manipulation and coordination of these components to build AI applications that can process and respond to data in versatile ways, including text, audio, images, and video. Through LCEL, developers can harness the full capabilities of the LangChain ecosystem to construct sophisticated AI solutions with relative ease.',\n",
       " 'contexts': [\"page_content='Chat models: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\nMessages: The unit of communication in chat models, used to represent model input and output.\\nChat history: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\nTools: A function with an associated schema defining the function's name, description, and the arguments it accepts.\\nTool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\nStructured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\nMemory: Information about a conversation that is persisted so that it can be used in future conversations.\\nMultimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\\nDocument loaders: Load a source as a list of documents.\\nRetrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.' metadata={'content_type': 'text/html; charset=utf-8', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'Conceptual guide | \\\\uf8ffü¶úÔ∏è\\\\uf8ffüîó LangChain'}\",\n",
       "  \"page_content='Conceptual guide | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain' metadata={'content_type': 'text/html; charset=utf-8', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'Conceptual guide | \\\\uf8ffü¶úÔ∏è\\\\uf8ffüîó LangChain'}\",\n",
       "  \"page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to' metadata={'content_type': 'text/html; charset=utf-8', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'Conceptual guide | \\\\uf8ffü¶úÔ∏è\\\\uf8ffüîó LangChain'}\",\n",
       "  'page_content=\\'LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store\\' metadata={\\'content_type\\': \\'text/html; charset=utf-8\\', \\'description\\': \\'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\', \\'language\\': \\'en\\', \\'source\\': \\'https://python.langchain.com/docs/expression_language/\\', \\'title\\': \\'Conceptual guide | \\\\uf8ffü¶úÔ∏è\\\\uf8ffüîó LangChain\\'}']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LangChain Expression Language (LCEL) is a syntax designed for orchestrating LangChain components. It is particularly useful for creating simpler applications within the LangChain framework. LCEL enables developers to define and execute complex operations involving various LangChain components, such as document loaders, retrieval systems, chat models, and tools, in a more streamlined and efficient manner. This syntax facilitates the manipulation and coordination of these components to build AI applications that can process and respond to data in versatile ways, including text, audio, images, and video. Through LCEL, developers can harness the full capabilities of the LangChain ecosystem to construct sophisticated AI solutions with relative ease.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG EValuators. \n",
    "\n",
    "components of Evalution pipeline.\n",
    "1. create a dataset \n",
    "* Datasets are collections of Examples, the core building block for the evaluation workflow in LangSmith. Examples provide the inputs over which you will be running your pipeline, and, if applicable, the outputs that you will be comparing against. All examples in a given dataset should follow the same schema. Examples contain an \"inputs\" dict and an \"output\" dict, along with (optionally) a metadata dict.\n",
    "* either using an llm\n",
    "* create a dictionary with quesionas and answers and convert it as a dataset\n",
    "* create a dataset using user provided examples or feedback\n",
    "2. EValuator \n",
    "* The inputs to an evaluator consist of:\n",
    "\n",
    "An Example - the inputs for your pipeline and optionally the reference outputs or labels\n",
    "A Run - observed output gathered from running the inputs through the Task\n",
    "An evaluator will then return an EvaluationResult (or similarly shaped dictionary), which is made up of:\n",
    "\n",
    "    key: The name the metric being evaluated\n",
    "    score: The value of the metric on this example\n",
    "    comment: the reasoning trajectory or other string information motivating the score\n",
    "\n",
    "* we can create a custom evaluator. see langchain  documentation for details\n",
    "* we can use lanchains off-shell-evaluator functions. \n",
    "* most of the times we use `LLM-as-judge` to comapre questions and answers  to evaluate and it langchain has many evaluator fucntions like QA, cot_qa, Criteria, labeld criteria and many more.\n",
    "3. Task. each example is processed through the task using an llm.\n",
    "4. Applying evals \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function client.create_examples() is part of the LangSmith SDK (Software Development Kit). It's used to create and add examples to a dataset in LangSmith for evaluating and monitoring your LLM (Large Language Model) applications.\n",
    "\n",
    "Here's a breakdown of what this function does:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Create Examples: It allows you to programmatically create examples that will be used to test your LLM application. These examples typically consist of inputs (e.g., user prompts, questions) and optionally, expected outputs or reference answers.\n",
    "Add to Dataset: The examples created by this function are added to a specific dataset within your LangSmith project. Datasets help you organize and manage your evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset and adding it to the langsmith datasets.\n",
    "# never forget to configure all the api_keys required.\n",
    "#dataset name should be unique always.\n",
    "\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\",\n",
    "    \"How can I make the output of my LCEL chain a string?\",\n",
    "    \"How can I apply a custom function to one of the inputs of an LCEL chain?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Use RunnablePassthrough. from langchain_core.runnables import RunnableParallel, RunnablePassthrough; from langchain_core.prompts import ChatPromptTemplate; from langchain_openai import ChatOpenAI; prompt = ChatPromptTemplate.from_template('Tell a joke about: {input}'); model = ChatOpenAI(); runnable = ({'input' : RunnablePassthrough()} | prompt | model); runnable.invoke('flowers')\",\n",
    "    \"Use StrOutputParser. from langchain_openai import ChatOpenAI; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.output_parsers import StrOutputParser; prompt = ChatPromptTemplate.from_template('Tell me a short joke about {topic}'); model = ChatOpenAI(model='gpt-3.5-turbo') #gpt-4 or other LLMs can be used here; output_parser = StrOutputParser(); chain = prompt | model | output_parser\",\n",
    "    \"Use RunnableLambda with itemgetter to extract the relevant key. from operator import itemgetter; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.runnables import RunnableLambda; from langchain_openai import ChatOpenAI; def length_function(text): return len(text); chain = ({'prompt_input': itemgetter('foo') | RunnableLambda(length_function),} | prompt | model); chain.invoke({'foo':'hello world'})\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"practice_erag_evaluation_dataset\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?',\n",
       "  'answer': \"Use RunnablePassthrough. from langchain_core.runnables import RunnableParallel, RunnablePassthrough; from langchain_core.prompts import ChatPromptTemplate; from langchain_openai import ChatOpenAI; prompt = ChatPromptTemplate.from_template('Tell a joke about: {input}'); model = ChatOpenAI(); runnable = ({'input' : RunnablePassthrough()} | prompt | model); runnable.invoke('flowers')\"},\n",
       " {'question': 'How can I make the output of my LCEL chain a string?',\n",
       "  'answer': \"Use StrOutputParser. from langchain_openai import ChatOpenAI; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.output_parsers import StrOutputParser; prompt = ChatPromptTemplate.from_template('Tell me a short joke about {topic}'); model = ChatOpenAI(model='gpt-3.5-turbo') #gpt-4 or other LLMs can be used here; output_parser = StrOutputParser(); chain = prompt | model | output_parser\"},\n",
       " {'question': 'How can I apply a custom function to one of the inputs of an LCEL chain?',\n",
       "  'answer': \"Use RunnableLambda with itemgetter to extract the relevant key. from operator import itemgetter; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.runnables import RunnableLambda; from langchain_openai import ChatOpenAI; def length_function(text): return len(text); chain = ({'prompt_input': itemgetter('foo') | RunnableLambda(length_function),} | prompt | model); chain.invoke({'foo':'hello world'})\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator.\n",
    " Lets consider the chain retrieved answert to  a reference answer.\n",
    "\n",
    " we use `LLM-as-Judge`  (`cot_qa` one of the kind ) as evaluator which gives us cot contextual accuracy.\n",
    "\n",
    " This evaluator uses a well predefined prompt. we can get that prompt from langchain hub\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"langchain-ai/cot_qa\")\n",
    "\n",
    "\n",
    "each langchainstringevaluator() takes 3 parameters. \n",
    "* questioin from dataset-> input defined in the dataset\n",
    "* answer(rag chain answer or response of the retriever)\n",
    "* reference(the ground truth) or answer from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "\n",
    "def predict_rag_answer(examples:dict):\n",
    "    \"\"\" use this for answer evalution. response produced by rag chain to the input question from dataset.\"\"\"\n",
    "    response = rag_bot.get_answer(examples['question'])\n",
    "    return {'answwer': response['answer']}\n",
    "\n",
    "def predict_rag_answe_with_context(examples:dict):\n",
    "    response = rag_bot.get_answer(examples['question'])\n",
    "    return {'answer':response['answer'],'contexts': response['contexts']}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RAG chain\n",
    "# dataset is collections of examples. each input is an example. \n",
    "# so, here predict_rag_answer takes an example which is a input in the dataset and send it to retriever to retireve the output.\n",
    "# predict_rag_answer return the only answer part of response which is a dict ype.\n",
    "\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "\n",
    "# predict_rag_answer_with_context also do the same thing. and it return the contexts aloing with answer from the response.\n",
    "# response is a dict with keys answer, contexts, \n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15512\\anaconda3\\envs\\dlai_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-6b06aa9d' at:\n",
      "https://smith.langchain.com/o/0d3dfbd3-48b9-4b11-9c49-27d343c90e06/datasets/5204bc66-d277-4dfe-aae5-1b4d1259d56a/compare?selectedSessions=4ba625b2-65d4-4b02-8ddd-25f2610a5896\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:24,  8.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evalulator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        },\n",
    "    )\n",
    "]\n",
    "dataset_name = \"practice_erag_evaluation_dataset\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"rag-qa-oai\",\n",
    "    metadata={\"variant\": \"LCEL context, gpt-3.5-turbo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type 2: Answer Hallucination\n",
    "# In this scenario, we gonna compare Rag chain answer to the retrieved documents.\n",
    "# to comapre rag chain answers and retrieved documents we commonly use Criteria as evaluator\n",
    "# we will use labeled_Score_string as LLM-as-Judge evaluator.\n",
    "\n",
    "# here we need only two inputs. one is rag chain answer and the ither is retrieved documents(context)\n",
    "\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "answer_hallucination_evaluator = LangChainStringEvaluator(\n",
    "    'labeled_score_string',\n",
    "    config ={\n",
    "        \"criteria\":{\n",
    "            'accuracy':\"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? A score of [[1]] means that the\n",
    "            Assistant answer contains is not at all based upon / grounded in the Groun Truth documentation. A score of [[5]] means \n",
    "            that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth \n",
    "            documentation. A score of [[10]] means that the Assistant answer is fully based upon the in the Ground Truth documentation.\"\"\"\n",
    "        },\n",
    "        'normalize_by': 10,\n",
    "    },\n",
    "    prepare_data= lambda run, examples:{\n",
    "        'prediction':run.outputs['answer'],\n",
    "        'reference': run. outputs['contexts'],\n",
    "        'input': examples.inputs['question'],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-hallucination-7bc8b1aa' at:\n",
      "https://smith.langchain.com/o/0d3dfbd3-48b9-4b11-9c49-27d343c90e06/datasets/5204bc66-d277-4dfe-aae5-1b4d1259d56a/compare?selectedSessions=4b299e61-245c-4bd3-ba96-b614b8f58a8b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:30, 10.32s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"practice_erag_evaluation_dataset\"\n",
    "experiment_results = evaluate(\n",
    "\n",
    "    predict_rag_answer_with_context,\n",
    "    data = dataset_name,\n",
    "    evaluators = [answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-3.5-turbo\",}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
