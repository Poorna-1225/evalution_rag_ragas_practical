{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangSmith, a **Run** represents a single execution of your LLM application (chain, agent, or model) with a specific input. It captures not only the final output but also the intermediate steps involved in generating that output. This detailed trace allows you to understand how your application arrived at its answer.\n",
    "\n",
    "An **Example** in LangSmith is a unit of data used to evaluate your LLM application. It consists of:\n",
    "\n",
    "* **inputs**: The data provided to your application.\n",
    "* **outputs** (optional): The expected or ideal output for the given input. Used by evaluators for comparison.\n",
    "* **metadata** (optional): Additional information about the example, useful for analysis or filtering.\n",
    "\n",
    "**How Runs and Examples Work Together in Evaluation**\n",
    "\n",
    "When you run an evaluation in LangSmith, you essentially execute your LLM application on a set of examples (often organized into a dataset). Each execution of your application on a single example generates a Run.  Evaluators then analyze these runs, comparing the actual outputs with the expected outputs (if provided) and any intermediate steps to assess the performance of your application.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's say you have an LLM application that summarizes news articles.\n",
    "\n",
    "* **Example:**\n",
    "    * **inputs**: { \"article\": \"The quick brown fox jumps over the lazy dog.\" }\n",
    "    * **outputs**: { \"summary\": \"A fox jumped over a dog.\" } \n",
    "\n",
    "* **Run:** When you execute your application with this example, a Run is created. It might contain:\n",
    "    * The original article\n",
    "    * Intermediate steps: perhaps the key phrases extracted, or the draft summaries generated along the way\n",
    "    * The final summary produced by your application (e.g., \"A quick brown fox jumped over a lazy dog.\")\n",
    "\n",
    "* **Evaluator:**  An evaluator might compare the generated summary in the Run with the expected summary in the Example to calculate a score, such as ROUGE score, to measure the similarity.\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* Runs provide detailed execution information, enabling in-depth analysis of your application's behavior.\n",
    "* Examples serve as test cases for your application, allowing you to systematically assess its performance.\n",
    "* Evaluators leverage the information from Runs and Examples to provide quantitative and qualitative feedback on your application's capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "LANGCHAIN_TRACING_V2 = 'true'\n",
    "LANGCHAIN_PROJECT = os.getenv('LANGCHAIN_PROJECT')\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "import openai\n",
    "from langsmith.wrappers import wrap_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(api_key=LANGCHAIN_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "# we are creating a dataset which takes input and outputs. each input is mapped with each output\n",
    "\n",
    "\n",
    "dataset_name = \"practice-dataset\"\n",
    "dataset =  client.create_dataset(dataset_name, description = \"Quick start evaluation test\")\n",
    "client.create_examples(\n",
    "    inputs = [\n",
    "        {'question': 'a rap bottle between atticus finch and cicero'},\n",
    "        {'question': 'a rap bottle between barbie and oppenheimer'} \n",
    "              ],\n",
    "    outputs = [\n",
    "        {'must_mention': ['lawyer','justice']},\n",
    "        {'must_mention': ['plastic','nuclear']}\n",
    "    ],\n",
    "    dataset_id = dataset.id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = wrap_openai(openai.Client(api_key=OPENAI_API_KEY))\n",
    "\n",
    "def predict(inputs:dict)->dict:\n",
    "  messages = [{'role':'user', 'content':inputs['question']}]\n",
    "  response = openai_client.chat.completions.create(messages = messages, model =\"gpt-3.5-turbo\")\n",
    "  return {'output':response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def must_mention(run:Run, example:Example)->dict:\n",
    "  prediction = run.outputs.get('output') or \"\"\n",
    "  required = example.outputs.get('must_attention') or []\n",
    "  score = all(phrase in prediction for phrase in required)\n",
    "  \n",
    "  return {'key':'must_mention', \"score\":score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15512\\anaconda3\\envs\\dlai_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rap-generator-43feb476' at:\n",
      "https://smith.langchain.com/o/0c9ea6bd-99e6-4c8a-a3a7-1abffbccfcd3/datasets/b3adaca8-1711-41f4-a6d5-fb1d6c77a11f/compare?selectedSessions=faa9179b-ceed-4590-9bf1-abe242cd3708\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict, # Ai system or llm model\n",
    "    data = dataset_name, # the data to predict and grade over\n",
    "    evaluators = [must_mention], # the evaluators to score the results\n",
    "    experiment_prefix = \"rap-generator\", # A prefix for your experiment names to easily identify them\n",
    "    metadata ={\n",
    "        'version':'1.0.0'\n",
    "    },\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
